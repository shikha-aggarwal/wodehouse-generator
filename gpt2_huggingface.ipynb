{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_huggingface_trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikha-aggarwal/wodehouse-generator/blob/main/gpt2_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmGk5DygmiUM"
      },
      "source": [
        "##### Check GPU allocated on Google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBxbjjjsqXJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b3f6be-9da3-454c-b36d-d38edb645486"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jan 28 17:53:08 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMz8UU3-Nez7"
      },
      "source": [
        "#### Step 1. Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11099652-3294-4943-fad0-8d7ff46a5bfd"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install git+https://github.com/huggingface/datasets\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 60874 (delta 12), reused 12 (delta 2), pack-reused 60840\u001b[K\n",
            "Receiving objects: 100% (60874/60874), 45.64 MiB | 28.00 MiB/s, done.\n",
            "Resolving deltas: 100% (42991/42991), done.\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-f6jexgoy\n",
            "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-f6jexgoy\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==4.3.0.dev0 from git+https://github.com/huggingface/transformers in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (20.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (0.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==4.3.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.3.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.3.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==4.3.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.3.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.3.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.3.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.3.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==4.3.0.dev0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==4.3.0.dev0) (2.4.7)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.3.0.dev0-cp36-none-any.whl size=1783713 sha256=dc9446b968eb32ffaf36acb048e1f101ed8ac32126541ebbeb04c8a03f1289b2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cgq3rpqu/wheels/70/d3/52/b3fa4f8b8ef04167ac62e5bb2accb62ae764db2a378247490e\n",
            "Successfully built transformers\n",
            "Collecting git+https://github.com/huggingface/datasets\n",
            "  Cloning https://github.com/huggingface/datasets to /tmp/pip-req-build-62vwjq2k\n",
            "  Running command git clone -q https://github.com/huggingface/datasets /tmp/pip-req-build-62vwjq2k\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (1.19.5)\n",
            "Collecting pyarrow>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/67/2f4fcce1b41bcc7e88a6bfdb42046597ae72e5bc95c2789b7c5ac893c433/pyarrow-3.0.0-cp36-cp36m-manylinux2014_x86_64.whl (20.7MB)\n",
            "\u001b[K     |████████████████████████████████| 20.7MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (4.41.1)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/73/826b19f3594756cb1c6c23d2fbd8ca6a77a9cd3b650c9dec5acc85004c38/xxhash-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (242kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 52.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.70.11.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (0.8)\n",
            "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.6/dist-packages (from datasets==1.2.1) (3.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.2.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets==1.2.1) (2018.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets==1.2.1) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib_metadata->datasets==1.2.1) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib_metadata->datasets==1.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.2.1) (1.15.0)\n",
            "Building wheels for collected packages: datasets\n",
            "  Building wheel for datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for datasets: filename=datasets-1.2.1-cp36-none-any.whl size=172662 sha256=18f6ce80c900e03fed6ffe562eac760fdce4eeec9651de05e5d789da06c92208\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-plxg915r/wheels/ef/76/f4/b4ebbfebddcfdccc0378b9d9c9332b141161feb1b31f8a17c7\n",
            "Successfully built datasets\n",
            "Installing collected packages: pyarrow, xxhash, fsspec, datasets\n",
            "  Found existing installation: pyarrow 0.14.1\n",
            "    Uninstalling pyarrow-0.14.1:\n",
            "      Successfully uninstalled pyarrow-0.14.1\n",
            "Successfully installed datasets-1.2.1 fsspec-0.8.5 pyarrow-3.0.0 xxhash-2.0.0\n",
            "tokenizers                    0.9.4          \n",
            "transformers                  4.3.0.dev0     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiZqyDhMH043"
      },
      "source": [
        "##### [Note: If you see errors during the above installation step, restarting the runtime might help.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYVih7yYmmi7"
      },
      "source": [
        "#### Step 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLHF8SNi6QDJ"
      },
      "source": [
        "# from changed directory\n",
        "# import run_language_modeling  \n",
        "# from transformers.examples import run_generation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23bb8f19-a508-4e55-f64a-f6afa0e29e30"
      },
      "source": [
        "# standard ML imports\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead\n",
        "\n",
        "# Text processing\n",
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from itertools import chain\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8PJVPNXmY92"
      },
      "source": [
        "#### Step 3. Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHQJYzKzZiIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55dbb028-0f34-460c-ed12-4053dc170b4b"
      },
      "source": [
        "# I am running on Colab with data stored in Google drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MIoc4RXT0k"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "##### Using sliding window of 8 sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA_0gVDRTBKY"
      },
      "source": [
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
        "sent_tokenize = sentence_tokenizer.tokenize\n",
        "\n",
        "\n",
        "def flatten(iterable):\n",
        "    return chain.from_iterable(iterable)\n",
        "\n",
        "\n",
        "def preprocess_book(book_txt):\n",
        "    start_idx = book_txt.index(\"START OF THIS PROJECT GUTENBERG\") + 100\n",
        "    end_idx = book_txt.index(\"END OF THIS PROJECT\") - 20\n",
        "    txt =  book_txt[start_idx: end_idx]\n",
        "    return re.sub(\"\\s+\", \" \", txt)\n",
        "\n",
        "\n",
        "def process_book(book_path):\n",
        "    try:\n",
        "        txt = preprocess_book(Path(book_path).read_text(\"utf-8\"))\n",
        "        sentences = [s for s in sent_tokenize(txt) if len(s) >= 16]\n",
        "        windowed_sentences = []\n",
        "        for snt in range(len(sentences)):\n",
        "            windowed_sentences.append(\" \".join(sentences[snt: snt + 8]))\n",
        "        return windowed_sentences\n",
        "    except:\n",
        "        print(f\"Could not parse \\n{book_path}\\n\")\n",
        "        return []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJMdA9R1bWkG"
      },
      "source": [
        "# Uncomment on first run ONLY. Once you have the training file, comment it out again.\n",
        "\n",
        "# train_data_directory = '/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/all_novels/'\n",
        "# sliding_train_data = '/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/train_sliding.txt'\n",
        "\n",
        "# books = []\n",
        "# for filename in os.listdir(train_data_directory):\n",
        "#   file_path = os.path.join(train_data_directory, filename)\n",
        "#   books.append(file_path)\n",
        "\n",
        "# buffer, BUFFER_SIZE = [], 100000\n",
        "# with open(sliding_train_data, \"w\") as file:\n",
        "#   for i, sentence in enumerate(flatten(process_book(f) for f in books)):\n",
        "#     if len(buffer) >= BUFFER_SIZE:\n",
        "#       file.write(\"\\n\".join(buffer))\n",
        "#       buffer.clear()\n",
        "#       print(i, end=\"\\r\")\n",
        "#     buffer.append(sentence)\n",
        "#   if len(buffer) > 0:\n",
        "#     file.write(\"\\n\".join(buffer))\n",
        "#     buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbNQ-dOgdUWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c8fb9f-4c40-456d-d669-2e79b15b75ae"
      },
      "source": [
        "!head /content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "uck Greif and the Online Distributed Proofreading Team INDISCRETIONS OF ARCHIE By P. G. Wodehouse It wasn't Archie's fault really. Its true he went to America and fell in love with Lucille, the daughter of a millionaire hotel proprietor and if he did marry her--well, what else was there to do? From his point of view, the whole thing was a thoroughly good egg; but Mr. Brewster, his father-in-law, thought differently, Archie had neither money nor occupation, which was distasteful in the eyes of the industrious Mr. Brewster; but the real bar was the fact that he had once adversely criticised one of his hotels. Archie does his best to heal the breach; but, being something of an ass, genus priceless, he finds it almost beyond his powers to placate “the man-eating fish” whom Providence has given him as a father-in-law P. G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you.\n",
            "G. Wodehouse It wasn't Archie's fault really. Its true he went to America and fell in love with Lucille, the daughter of a millionaire hotel proprietor and if he did marry her--well, what else was there to do? From his point of view, the whole thing was a thoroughly good egg; but Mr. Brewster, his father-in-law, thought differently, Archie had neither money nor occupation, which was distasteful in the eyes of the industrious Mr. Brewster; but the real bar was the fact that he had once adversely criticised one of his hotels. Archie does his best to heal the breach; but, being something of an ass, genus priceless, he finds it almost beyond his powers to placate “the man-eating fish” whom Providence has given him as a father-in-law P. G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this?\n",
            "Its true he went to America and fell in love with Lucille, the daughter of a millionaire hotel proprietor and if he did marry her--well, what else was there to do? From his point of view, the whole thing was a thoroughly good egg; but Mr. Brewster, his father-in-law, thought differently, Archie had neither money nor occupation, which was distasteful in the eyes of the industrious Mr. Brewster; but the real bar was the fact that he had once adversely criticised one of his hotels. Archie does his best to heal the breach; but, being something of an ass, genus priceless, he finds it almost beyond his powers to placate “the man-eating fish” whom Providence has given him as a father-in-law P. G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications.\n",
            "From his point of view, the whole thing was a thoroughly good egg; but Mr. Brewster, his father-in-law, thought differently, Archie had neither money nor occupation, which was distasteful in the eyes of the industrious Mr. Brewster; but the real bar was the fact that he had once adversely criticised one of his hotels. Archie does his best to heal the breach; but, being something of an ass, genus priceless, he finds it almost beyond his powers to placate “the man-eating fish” whom Providence has given him as a father-in-law P. G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him.\n",
            "Archie does his best to heal the breach; but, being something of an ass, genus priceless, he finds it almost beyond his powers to placate “the man-eating fish” whom Providence has given him as a father-in-law P. G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it.\n",
            "G. Wodehouse AUTHOR OF “THE LITTLE WARRIOR,” “A DAMSEL IN DISTRESS,” “UNEASY MONEY,” ETC. NEW YORK GEORGE H. DORAN COMPANY COPYRIGHT,1921, BY GEORGE H, DORAN COMPANY COPYRIGHT, 1920, BY INTERNATIONAL MAGAZINE COMPANY (COSMOPOLITAN MAGAZINE) PRINTED IN THE UNITED STATES OF AMERICA DEDICATION TO B. W. KING-HALL My dear Buddy,-- We have been friends for eighteen years. A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it. However, I can't imagine anyone quarrelling with you, and I am getting more attractive all the time, so let's take a chance.\n",
            "A considerable proportion of my books were written under your hospitable roof. And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it. However, I can't imagine anyone quarrelling with you, and I am getting more attractive all the time, so let's take a chance. CONTENTS I DISTRESSING SCENE IN A HOTEL II A SHOCK FOR MR. BREWSTER III MR. BREWSTER DELIVERS SENTENCE IV WORK WANTED V STRANGE EXPERIENCE OF AN ARTIST'S MODEL VI THE BOMB VII MR. ROSCOE SHERRIFF HAS AN IDEA VIII A DISTURBED NIGHT FOR DEAR OLD SQUIFFY IX A LETTER FROM PARKER X DOING FATHER A BIT OF GOOD XI SALVATORE CHOOSES THE WRONG MOMENT XII BRIGHT EYES-AND A FLY XIII RALLYING ROUND PERCY XIV THE SAD CASE OF LOONEY BIDDLE XV SUMMER STORMS XVI ARCHIE ACCEPTS A SITUATION XVII BROTHER BILL'S ROMANCE XVIII THE SAUSAGE CHAPPIE XIX REGGIE COMES TO LIFE XX THE SAUSAGE CHAPPIE CLICKS XXI THE-GROWING BOY XXII WASHY STEPS INTO THE HALL OF FAME XXIII MOTHER'S-KNEE XXIV THE MELTING OF MR. CONNOLLY XXV THE WIGMORE VENUS XXVI A TALE OF A GRANDFATHER CHAPTER I. DISTRESSING SCENE “I say, laddie!” said Archie.\n",
            "And yet I have never dedicated one to you. What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it. However, I can't imagine anyone quarrelling with you, and I am getting more attractive all the time, so let's take a chance. CONTENTS I DISTRESSING SCENE IN A HOTEL II A SHOCK FOR MR. BREWSTER III MR. BREWSTER DELIVERS SENTENCE IV WORK WANTED V STRANGE EXPERIENCE OF AN ARTIST'S MODEL VI THE BOMB VII MR. ROSCOE SHERRIFF HAS AN IDEA VIII A DISTURBED NIGHT FOR DEAR OLD SQUIFFY IX A LETTER FROM PARKER X DOING FATHER A BIT OF GOOD XI SALVATORE CHOOSES THE WRONG MOMENT XII BRIGHT EYES-AND A FLY XIII RALLYING ROUND PERCY XIV THE SAD CASE OF LOONEY BIDDLE XV SUMMER STORMS XVI ARCHIE ACCEPTS A SITUATION XVII BROTHER BILL'S ROMANCE XVIII THE SAUSAGE CHAPPIE XIX REGGIE COMES TO LIFE XX THE SAUSAGE CHAPPIE CLICKS XXI THE-GROWING BOY XXII WASHY STEPS INTO THE HALL OF FAME XXIII MOTHER'S-KNEE XXIV THE MELTING OF MR. CONNOLLY XXV THE WIGMORE VENUS XXVI A TALE OF A GRANDFATHER CHAPTER I. DISTRESSING SCENE “I say, laddie!” said Archie. “Sir?” replied the desk-clerk alertly.\n",
            "What will be the verdict of Posterity on this? The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it. However, I can't imagine anyone quarrelling with you, and I am getting more attractive all the time, so let's take a chance. CONTENTS I DISTRESSING SCENE IN A HOTEL II A SHOCK FOR MR. BREWSTER III MR. BREWSTER DELIVERS SENTENCE IV WORK WANTED V STRANGE EXPERIENCE OF AN ARTIST'S MODEL VI THE BOMB VII MR. ROSCOE SHERRIFF HAS AN IDEA VIII A DISTURBED NIGHT FOR DEAR OLD SQUIFFY IX A LETTER FROM PARKER X DOING FATHER A BIT OF GOOD XI SALVATORE CHOOSES THE WRONG MOMENT XII BRIGHT EYES-AND A FLY XIII RALLYING ROUND PERCY XIV THE SAD CASE OF LOONEY BIDDLE XV SUMMER STORMS XVI ARCHIE ACCEPTS A SITUATION XVII BROTHER BILL'S ROMANCE XVIII THE SAUSAGE CHAPPIE XIX REGGIE COMES TO LIFE XX THE SAUSAGE CHAPPIE CLICKS XXI THE-GROWING BOY XXII WASHY STEPS INTO THE HALL OF FAME XXIII MOTHER'S-KNEE XXIV THE MELTING OF MR. CONNOLLY XXV THE WIGMORE VENUS XXVI A TALE OF A GRANDFATHER CHAPTER I. DISTRESSING SCENE “I say, laddie!” said Archie. “Sir?” replied the desk-clerk alertly. All the employes of the Hotel Cosmopolis were alert.\n",
            "The fact is, I have become rather superstitious about dedications. No sooner do you label a book with the legend-- TO MY BEST FRIEND X than X cuts you in Piccadilly, or you bring a lawsuit against him. There is a fatality about it. However, I can't imagine anyone quarrelling with you, and I am getting more attractive all the time, so let's take a chance. CONTENTS I DISTRESSING SCENE IN A HOTEL II A SHOCK FOR MR. BREWSTER III MR. BREWSTER DELIVERS SENTENCE IV WORK WANTED V STRANGE EXPERIENCE OF AN ARTIST'S MODEL VI THE BOMB VII MR. ROSCOE SHERRIFF HAS AN IDEA VIII A DISTURBED NIGHT FOR DEAR OLD SQUIFFY IX A LETTER FROM PARKER X DOING FATHER A BIT OF GOOD XI SALVATORE CHOOSES THE WRONG MOMENT XII BRIGHT EYES-AND A FLY XIII RALLYING ROUND PERCY XIV THE SAD CASE OF LOONEY BIDDLE XV SUMMER STORMS XVI ARCHIE ACCEPTS A SITUATION XVII BROTHER BILL'S ROMANCE XVIII THE SAUSAGE CHAPPIE XIX REGGIE COMES TO LIFE XX THE SAUSAGE CHAPPIE CLICKS XXI THE-GROWING BOY XXII WASHY STEPS INTO THE HALL OF FAME XXIII MOTHER'S-KNEE XXIV THE MELTING OF MR. CONNOLLY XXV THE WIGMORE VENUS XXVI A TALE OF A GRANDFATHER CHAPTER I. DISTRESSING SCENE “I say, laddie!” said Archie. “Sir?” replied the desk-clerk alertly. All the employes of the Hotel Cosmopolis were alert. It was one of the things on which Mr. Daniel Brewster, the proprietor, insisted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLLJSU7nmqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39677b65-19ca-430d-baf4-9a53ee069872"
      },
      "source": [
        "# number of lines, words, characters respectively\n",
        "\n",
        "!wc /content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  170036 18024409 99800308 /content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/train_sliding.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV"
      },
      "source": [
        "## TAKES LOOOONG TIME. DO NOT RUN ONCE you have a trained model handy.\n",
        "\n",
        "# !python run_language_modeling.py \\\n",
        "#     --output_dir='/content/drive/My Drive/finetuned_models/wodehouse' \\\n",
        "#     --model_type=gpt2 \\\n",
        "#     --model_name_or_path=gpt2-medium \\\n",
        "#     --save_total_limit=5 \\\n",
        "#     --num_train_epochs=1.0 \\\n",
        "#     --do_train \\\n",
        "#     --evaluate_during_training \\\n",
        "#     --logging_steps=500 \\\n",
        "#     --save_steps=1500 \\\n",
        "#     --train_data_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt \\\n",
        "#     --do_eval \\\n",
        "#     --eval_data_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/validate.txt \\\n",
        "#     --per_gpu_train_batch_size=2 \\\n",
        "#     --per_gpu_eval_batch_size=2 \\\n",
        "#     --block_size=128 \\\n",
        "#     --gradient_accumulation_steps=5 \\\n",
        "#     --overwrite_output_dir # too lazy to delete previous failed run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6b6tV9CI7cS",
        "outputId": "4d6cff5d-710a-4140-b9b5-70c3627bc735"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data  transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCnuV_rjId-n",
        "outputId": "c003f978-ffa3-4503-cbb9-c48ead52da54"
      },
      "source": [
        "## TAKES LOOOONG TIME. DO NOT RUN ONCE you have a trained model handy.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/transformers/examples/language-modeling')\n",
        "\n",
        "!python run_clm.py \\\n",
        "    --output_dir='/content/drive/My Drive/finetuned_models_2/wodehouse' \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=gpt2-medium \\\n",
        "    --save_total_limit=5 \\\n",
        "    --num_train_epochs=1.0 \\\n",
        "    --do_train \\\n",
        "    --logging_steps=500 \\\n",
        "    --save_steps=1500 \\\n",
        "    --train_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt \\\n",
        "    --do_eval=y \\\n",
        "    --validation_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/validate.txt \\\n",
        "    --per_gpu_train_batch_size=2 \\\n",
        "    --per_gpu_eval_batch_size=2 \\\n",
        "    --block_size=128 \\\n",
        "    --gradient_accumulation_steps=5 \\\n",
        "    --overwrite_output_dir # too lazy to delete previous failed run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-28 18:13:44.368248: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "01/28/2021 18:13:45 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/28/2021 18:13:45 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir=/content/drive/My Drive/finetuned_models_2/wodehouse, overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=EvaluationStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=5, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Jan28_18-13-45_c65b84e3a59f, logging_first_step=False, logging_steps=500, save_steps=1500, save_total_limit=5, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=/content/drive/My Drive/finetuned_models_2/wodehouse, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, pin_memory=True, _n_gpu=1)\n",
            "Using custom data configuration default-3155a570d63d4eac\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-3155a570d63d4eac/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-3155a570d63d4eac/0.0.0/44d63bd03e7e554f16131765a251f2d8333a5fe8a73f6ea3de012dbc49443691. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1286] 2021-01-28 18:13:48,017 >> https://huggingface.co/gpt2-medium/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpivr4cfjs\n",
            "Downloading: 100% 718/718 [00:00<00:00, 662kB/s]\n",
            "[INFO|file_utils.py:1290] 2021-01-28 18:13:48,223 >> storing https://huggingface.co/gpt2-medium/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|file_utils.py:1293] 2021-01-28 18:13:48,224 >> creating metadata file for /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:445] 2021-01-28 18:13:48,224 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:481] 2021-01-28 18:13:48,225 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:445] 2021-01-28 18:13:48,428 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff\n",
            "[INFO|configuration_utils.py:481] 2021-01-28 18:13:48,429 >> Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.3.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1286] 2021-01-28 18:13:48,643 >> https://huggingface.co/gpt2-medium/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuita0fq5\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 2.77MB/s]\n",
            "[INFO|file_utils.py:1290] 2021-01-28 18:13:49,233 >> storing https://huggingface.co/gpt2-medium/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1293] 2021-01-28 18:13:49,233 >> creating metadata file for /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1286] 2021-01-28 18:13:49,450 >> https://huggingface.co/gpt2-medium/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmxa_lo8t\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 1.40MB/s]\n",
            "[INFO|file_utils.py:1290] 2021-01-28 18:13:49,996 >> storing https://huggingface.co/gpt2-medium/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1293] 2021-01-28 18:13:49,996 >> creating metadata file for /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1286] 2021-01-28 18:13:50,220 >> https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbaoyl4ew\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 3.49MB/s]\n",
            "[INFO|file_utils.py:1290] 2021-01-28 18:13:50,827 >> storing https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1293] 2021-01-28 18:13:50,827 >> creating metadata file for /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1783] 2021-01-28 18:13:50,827 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1783] 2021-01-28 18:13:50,827 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1783] 2021-01-28 18:13:50,828 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1286] 2021-01-28 18:13:51,111 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5xh8kchk\n",
            "Downloading: 100% 1.52G/1.52G [00:25<00:00, 60.2MB/s]\n",
            "[INFO|file_utils.py:1290] 2021-01-28 18:14:16,588 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|file_utils.py:1293] 2021-01-28 18:14:16,588 >> creating metadata file for /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1027] 2021-01-28 18:14:16,588 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1\n",
            "[INFO|modeling_utils.py:1143] 2021-01-28 18:14:32,690 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1152] 2021-01-28 18:14:32,690 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "100% 171/171 [00:29<00:00,  5.73ba/s]\n",
            "100% 16/16 [00:00<00:00, 37.55ba/s]\n",
            "100% 171/171 [06:54<00:00,  2.43s/ba]\n",
            "100% 16/16 [00:00<00:00, 19.33ba/s]\n",
            "[INFO|trainer.py:429] 2021-01-28 18:22:08,070 >> The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[INFO|trainer.py:429] 2021-01-28 18:22:08,071 >> The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: .\n",
            "[WARNING|training_args.py:485] 2021-01-28 18:22:08,072 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:485] 2021-01-28 18:22:08,078 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[INFO|trainer.py:832] 2021-01-28 18:22:08,078 >> ***** Running training *****\n",
            "[INFO|trainer.py:833] 2021-01-28 18:22:08,078 >>   Num examples = 186911\n",
            "[INFO|trainer.py:834] 2021-01-28 18:22:08,078 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:835] 2021-01-28 18:22:08,078 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:836] 2021-01-28 18:22:08,078 >>   Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "[INFO|trainer.py:837] 2021-01-28 18:22:08,078 >>   Gradient Accumulation steps = 5\n",
            "[INFO|trainer.py:838] 2021-01-28 18:22:08,079 >>   Total optimization steps = 18691\n",
            "[WARNING|training_args.py:485] 2021-01-28 18:22:08,085 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "[WARNING|training_args.py:499] 2021-01-28 18:22:08,085 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "{'loss': 3.1718, 'learning_rate': 4.8662457867422825e-05, 'epoch': 0.03}\n",
            "{'loss': 3.0199, 'learning_rate': 4.732491573484565e-05, 'epoch': 0.05}\n",
            "{'loss': 2.9462, 'learning_rate': 4.598737360226847e-05, 'epoch': 0.08}\n",
            "  8% 1500/18691 [19:05<3:37:38,  1.32it/s][INFO|trainer.py:1392] 2021-01-28 18:41:13,163 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-1500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 18:41:13,169 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 18:41:20,740 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.8972, 'learning_rate': 4.46498314696913e-05, 'epoch': 0.11}\n",
            "{'loss': 2.8563, 'learning_rate': 4.331228933711412e-05, 'epoch': 0.13}\n",
            "{'loss': 2.8199, 'learning_rate': 4.197474720453695e-05, 'epoch': 0.16}\n",
            " 16% 3000/18691 [38:45<3:19:14,  1.31it/s][INFO|trainer.py:1392] 2021-01-28 19:00:53,586 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-3000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 19:00:53,595 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 19:01:01,107 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.7817, 'learning_rate': 4.0637205071959763e-05, 'epoch': 0.19}\n",
            "{'loss': 2.7288, 'learning_rate': 3.929966293938259e-05, 'epoch': 0.21}\n",
            "{'loss': 2.7055, 'learning_rate': 3.7962120806805416e-05, 'epoch': 0.24}\n",
            " 24% 4500/18691 [58:30<2:59:36,  1.32it/s][INFO|trainer.py:1392] 2021-01-28 19:20:38,333 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-4500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 19:20:38,340 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 19:20:46,115 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 2.6735, 'learning_rate': 3.662457867422824e-05, 'epoch': 0.27}\n",
            "{'loss': 2.6506, 'learning_rate': 3.528703654165106e-05, 'epoch': 0.29}\n",
            "{'loss': 2.6134, 'learning_rate': 3.394949440907389e-05, 'epoch': 0.32}\n",
            " 32% 6000/18691 [1:18:24<2:41:45,  1.31it/s][INFO|trainer.py:1392] 2021-01-28 19:40:32,902 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-6000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 19:40:32,910 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 19:40:41,374 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 2.6019, 'learning_rate': 3.261195227649671e-05, 'epoch': 0.35}\n",
            "{'loss': 2.5751, 'learning_rate': 3.127441014391954e-05, 'epoch': 0.37}\n",
            "{'loss': 2.5538, 'learning_rate': 2.9936868011342358e-05, 'epoch': 0.4}\n",
            " 40% 7500/18691 [1:38:20<2:21:21,  1.32it/s][INFO|trainer.py:1392] 2021-01-28 20:00:28,281 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-7500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 20:00:28,290 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 20:00:34,910 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-7500/pytorch_model.bin\n",
            "{'loss': 2.5392, 'learning_rate': 2.859932587876518e-05, 'epoch': 0.43}\n",
            "{'loss': 2.5166, 'learning_rate': 2.7261783746188007e-05, 'epoch': 0.45}\n",
            "{'loss': 2.4878, 'learning_rate': 2.5924241613610827e-05, 'epoch': 0.48}\n",
            " 48% 9000/18691 [1:58:11<2:05:02,  1.29it/s][INFO|trainer.py:1392] 2021-01-28 20:20:19,791 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-9000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 20:20:19,800 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 20:20:26,792 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 20:21:01,050 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-1500] due to args.save_total_limit\n",
            "{'loss': 2.4764, 'learning_rate': 2.4586699481033653e-05, 'epoch': 0.51}\n",
            "{'loss': 2.4632, 'learning_rate': 2.3249157348456476e-05, 'epoch': 0.54}\n",
            "{'loss': 2.4637, 'learning_rate': 2.1911615215879303e-05, 'epoch': 0.56}\n",
            " 56% 10500/18691 [2:18:05<1:45:16,  1.30it/s][INFO|trainer.py:1392] 2021-01-28 20:40:13,781 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-10500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 20:40:13,788 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 20:40:20,699 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 20:40:53,929 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-3000] due to args.save_total_limit\n",
            "{'loss': 2.4265, 'learning_rate': 2.0574073083302126e-05, 'epoch': 0.59}\n",
            "{'loss': 2.4094, 'learning_rate': 1.923653095072495e-05, 'epoch': 0.62}\n",
            "{'loss': 2.4108, 'learning_rate': 1.789898881814777e-05, 'epoch': 0.64}\n",
            " 64% 12000/18691 [2:38:08<1:24:56,  1.31it/s][INFO|trainer.py:1392] 2021-01-28 21:00:16,188 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-12000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 21:00:16,197 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 21:00:23,623 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 21:00:53,669 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-4500] due to args.save_total_limit\n",
            "{'loss': 2.3966, 'learning_rate': 1.6561446685570598e-05, 'epoch': 0.67}\n",
            "{'loss': 2.3813, 'learning_rate': 1.5223904552993421e-05, 'epoch': 0.7}\n",
            "{'loss': 2.3707, 'learning_rate': 1.3886362420416244e-05, 'epoch': 0.72}\n",
            " 72% 13500/18691 [2:58:05<1:05:55,  1.31it/s][INFO|trainer.py:1392] 2021-01-28 21:20:13,989 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-13500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 21:20:13,997 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 21:20:20,787 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 21:20:51,248 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-6000] due to args.save_total_limit\n",
            "{'loss': 2.3636, 'learning_rate': 1.2548820287839067e-05, 'epoch': 0.75}\n",
            "{'loss': 2.3558, 'learning_rate': 1.121127815526189e-05, 'epoch': 0.78}\n",
            "{'loss': 2.3479, 'learning_rate': 9.873736022684715e-06, 'epoch': 0.8}\n",
            " 80% 15000/18691 [3:18:01<47:12,  1.30it/s][INFO|trainer.py:1392] 2021-01-28 21:40:09,289 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-15000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 21:40:09,296 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 21:40:16,482 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 21:40:45,569 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-7500] due to args.save_total_limit\n",
            "{'loss': 2.333, 'learning_rate': 8.536193890107538e-06, 'epoch': 0.83}\n",
            "{'loss': 2.3191, 'learning_rate': 7.1986517575303625e-06, 'epoch': 0.86}\n",
            "{'loss': 2.3238, 'learning_rate': 5.861109624953186e-06, 'epoch': 0.88}\n",
            " 88% 16500/18691 [3:37:59<27:47,  1.31it/s][INFO|trainer.py:1392] 2021-01-28 22:00:07,943 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-16500\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 22:00:07,950 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 22:00:14,938 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 22:00:44,264 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-9000] due to args.save_total_limit\n",
            "{'loss': 2.3225, 'learning_rate': 4.52356749237601e-06, 'epoch': 0.91}\n",
            "{'loss': 2.3212, 'learning_rate': 3.186025359798834e-06, 'epoch': 0.94}\n",
            "{'loss': 2.3101, 'learning_rate': 1.8484832272216577e-06, 'epoch': 0.96}\n",
            " 96% 18000/18691 [3:57:58<08:44,  1.32it/s][INFO|trainer.py:1392] 2021-01-28 22:20:06,166 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-18000\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 22:20:06,174 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 22:20:13,301 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|trainer.py:1451] 2021-01-28 22:20:46,622 >> Deleting older checkpoint [/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-10500] due to args.save_total_limit\n",
            "{'loss': 2.3093, 'learning_rate': 5.109410946444814e-07, 'epoch': 0.99}\n",
            "100% 18691/18691 [4:07:32<00:00,  1.31it/s][INFO|trainer.py:998] 2021-01-28 22:29:40,193 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 14852.115, 'train_samples_per_second': 1.258, 'epoch': 1.0}\n",
            "100% 18691/18691 [4:07:32<00:00,  1.26it/s]\n",
            "[INFO|trainer.py:1392] 2021-01-28 22:29:40,249 >> Saving model checkpoint to /content/drive/My Drive/finetuned_models_2/wodehouse\n",
            "[INFO|configuration_utils.py:300] 2021-01-28 22:29:40,261 >> Configuration saved in /content/drive/My Drive/finetuned_models_2/wodehouse/config.json\n",
            "[INFO|modeling_utils.py:817] 2021-01-28 22:29:47,311 >> Model weights saved in /content/drive/My Drive/finetuned_models_2/wodehouse/pytorch_model.bin\n",
            "01/28/2021 22:29:47 - INFO - __main__ -   ***** Train results *****\n",
            "01/28/2021 22:29:47 - INFO - __main__ -     epoch = 1.0\n",
            "01/28/2021 22:29:47 - INFO - __main__ -     train_runtime = 14852.115\n",
            "01/28/2021 22:29:47 - INFO - __main__ -     train_samples_per_second = 1.258\n",
            "01/28/2021 22:29:47 - INFO - __main__ -   *** Evaluate ***\n",
            "[WARNING|training_args.py:499] 2021-01-28 22:29:47,446 >> Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
            "[INFO|trainer.py:1584] 2021-01-28 22:29:47,446 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:1585] 2021-01-28 22:29:47,447 >>   Num examples = 1260\n",
            "[INFO|trainer.py:1586] 2021-01-28 22:29:47,448 >>   Batch size = 2\n",
            "100% 630/630 [00:29<00:00, 21.71it/s]\n",
            "01/28/2021 22:30:16 - INFO - __main__ -   ***** Eval results *****\n",
            "01/28/2021 22:30:16 - INFO - __main__ -     perplexity = 36.58978545212275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "This section shows how to compute perplexity of a dataset according to either the pre-trained or your fine-tuned language model. While this is possible to do by calling `run_language_modeling.py` on the command-line as above, we'll instead call the Python functions directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSgGhtcDNd"
      },
      "source": [
        "#### Look at what checkpoints are available\n",
        "Run `ls` to look at what checkpoints saved been saved. You'll want to set `CHECKPOINT_PATH` below to one of these in order to evaluate the model weights saved in that checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_qHytBIETo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3e73b5-496e-40df-9ced-48d9f1243ea6"
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models_2/wodehouse'"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint-12000  config.json\t\t   tokenizer_config.json\n",
            "checkpoint-13500  eval_results_clm.txt\t   trainer_state.json\n",
            "checkpoint-15000  merges.txt\t\t   training_args.bin\n",
            "checkpoint-16500  pytorch_model.bin\t   train_results.txt\n",
            "checkpoint-18000  special_tokens_map.json  vocab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf"
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None\n",
        "  )\n",
        "  \n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  #args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb"
      },
      "source": [
        "#### How is the trained model doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsGfPtzJo6b"
      },
      "source": [
        "class DictToObject(object):\n",
        "\n",
        "    def __init__(self, dictionary):\n",
        "        def _traverse(key, element):\n",
        "            if isinstance(element, dict):\n",
        "                return key, DictToObject(element)\n",
        "            else:\n",
        "                return key, element\n",
        "\n",
        "        objd = dict(_traverse(k, v) for k, v in dictionary.items())\n",
        "        self.__dict__.update(objd)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCKSncEBYgJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "c6e0bc58-ab24-4a46-a15e-ee6168a18266"
      },
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models_2/wodehouse/checkpoint-18000'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/finetuned_models_2/wodehouse/output_checkpoint_18000'\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/validate.txt\",\n",
        "              \"/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=OUTPUT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = DictToObject(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/models/auto/modeling_auto.py:954: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1ccf297f61c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATA_PATHS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_perplexity_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'perplexity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   print('{} is the perplexity of {} according to {}'.format(\n",
            "\u001b[0;32m<ipython-input-25-7915d9bf41e4>\u001b[0m in \u001b[0;36mdo_perplexity_eval\u001b[0;34m(args, model, data_file_path)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m#args.block_size = min(args.block_size, tokenizer.max_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_language_modeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'run_language_modeling' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates text samples that are are continuations of a provided prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh"
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0"
      },
      "source": [
        "def generate_wodehouse_samples(prompt):\n",
        "\n",
        "  # You should try out other prompts as well as no prompt at all.\n",
        "  PROMPT = prompt\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  n_gpu = torch.cuda.device_count()\n",
        "  print(\"Running on device: \", device)\n",
        "\n",
        "  args = collections.defaultdict(\n",
        "    model_name_or_path=CHECKPOINT_PATH,\n",
        "    output_dir=OUTPUT_PATH,\n",
        "    n_gpu=n_gpu,\n",
        "    mlm=False,\n",
        "    device=device,\n",
        "    model_type='gpt2',\n",
        "    seed=42,\n",
        "    stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "    temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "    k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "    p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "    repetition_penalty=None,\n",
        "    length=900,  # Number of tokens to generate.\n",
        "    num_return_sequences=3,  # Number of independently computed samples to generate.\n",
        "  )\n",
        "  args = DictToObject(dict(args))\n",
        "\n",
        "  model = load_model(args)\n",
        "  sequences = generate_samples(args, model, PROMPT)\n",
        "\n",
        "  return sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENOalkcbqo3o"
      },
      "source": [
        "def print_sequences(sequences):\n",
        "\n",
        "  for idx, sequence in enumerate(sequences):\n",
        "  print('\\n====== GENERATION {} ======'.format(idx))\n",
        "  print(sequence)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFeDb23LNJO"
      },
      "source": [
        "sequences = generate_wodehouse_samples(\"Seated with his wife at breakfast on the veranda which overlooked the rolling lawns and leafy woods of his charming Sussex home, Geoffrey Windlebird, the great financier, was enjoying the morning sun to the full. \")\n",
        "print_sequences(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWgdXIZ-shY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a00d21e7-07d2-4e3a-a232-6350e79aa945"
      },
      "source": [
        "sequences = generate_wodehouse_samples(\"It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s.\")\n",
        "print_sequences(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/23/2020 21:16:17 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/config.json\n",
            "10/23/2020 21:16:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/23/2020 21:16:17 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/23/2020 21:16:30 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/config.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/added_tokens.json. We won't load it.\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/vocab.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/merges.txt\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/special_tokens_map.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/tokenizer_config.json\n",
            "10/23/2020 21:16:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. He was feeling that strange exhilaration, the thrill which comes to those who have done well at school, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores.\n",
            "All he had to do was to hang about and wait, and soon the other fellow would get busy and do it at him, and it would be his triumph. As for me, I would merely sit there. He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. He was feeling that strange exhilaration, the thrill which comes to those who have done well at school, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores. He was feeling that strange exhilaration, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores. It was only now that gradually he began to entertain tentative hopes that I might be of any assistance.\n",
            "As for me, I would merely sit there. He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that gradually he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray.\n",
            "He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that slowly he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray. The atmosphere of the place was heavy with the air of one with great affairs to discuss, and I was finding it difficult to follow what the manager was saying and to make eye- contact. With me, on the other hand, it seemed as if he were talking to himself. “And to look at you,” he proceeded as he reached for his tea, “I’m like a young Piccadilly toke. He has got no time for yapping at me.’llty old Bingo.’s got no time for yapping at you. And I’d have rather given up the struggle than have got engaged to _you_, if my lordship’s got his eye.\n",
            "In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that slowly he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray. The atmosphere of the place was heavy with the air of one with great affairs to discuss, and I was finding it difficult to follow what the manager was saying and to make eye- contact. With me, on the other hand, it seemed as if he were talking to himself. “And to look at you,” he proceeded as he reached for his tea, “I’m like a young Piccadilly toke. He\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. “Gimme dat, Liz!” He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it.\n",
            "“Gimme dat, Liz!” He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing.\n",
            "He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene.\n",
            "“A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food.\n",
            "“A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food. He took one of these little chats without speaking. There was a deep state of melancholy about Sally which he was trying not to show. His mind seemed in a confused state at his elbow, as if he were playing a deaf game.\n",
            "He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food. She took one of these little chats without speaking. “What do you mean by it?�\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed its expressionless stolidity. He seemed to regard me as something quite ordinary and uninteresting which he could invite to browse among his sundry wares. “What colour is that, Ukridge?” demanded the proprietor. You would mistake it for a rose, I suppose.” It might have been a rose, Ukridge supposed; but his eyes narrowed, as he contemplated the peculiar shape of one of those roses.\n",
            "Ukridge was in the street in front of Selfridge’s shop. Standing among a growing collection of elderly artists, he perceived me, as I entered, as a trifle dishevelled—a fact which, he, as we shook hands awkwardly, was due to the cold weather and the closeness of the hour, for I had only just returned from Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed its expressionless stolidity. He seemed to regard me as something quite ordinary and uninteresting which he could invite to browse among his sundry wares. “What colour is that, Ukridge?” demanded the proprietor. You would mistake it for a rose, I suppose.” There was a pause of some moments, during which Ukridge looked out of the window, stared a little blankly in perplexity into space, and finally spoke. “What colour is that, Mr. Ukridge?” “Oh, that was the colour of my hair.” Ukridge hesitated for a moment, then, with that peculiar delicacy of which he is so notoriously capable, uttered at a venture the words “You know me,” and waved me down with a sort of paternal gravity; for Ukridge, though he knew not me, was certainly a man of action. That night, at dinner, he had shown a tendency to sit up quite late, and he did so again now. It was, he told me, because he seemed to be dying to see his niece, whom he had not seen much of since her arrival, appear unexpectedly in person. He was feeling aggrieved and baffled. He had just received a telegram from my aunt describing my engagement to her, and it seemed to him that she would be glad to see me. At the moment when this young man had introduced himself to me, in the High Street before the grocer’s, he had seemed, from his pale, square face, almost cheerful; but there had been something troubling in his demeanour that night. When we shook hands, his pallor had changed; and a hard look had come into his eyes, which were set with that grim determination which marks the man of action. I have described. “Hallo, old horse!” he was saying, looking at me. I don’t know what sort of a dress this is.\n",
            "Standing among a growing collection of elderly artists, he perceived me, as I entered, as a trifle dishevelled—a fact which, he, as we shook hands awkwardly, was due to the cold weather and the closeness of the hour, for I had only just returned from Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7pDQ5pnHCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYYtkxo8TC3Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSDr0qITHfN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}