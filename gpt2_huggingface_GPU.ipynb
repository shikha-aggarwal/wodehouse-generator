{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt2_huggingface_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikha-aggarwal/wodehouse-generator/blob/main/gpt2_huggingface_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmGk5DygmiUM"
      },
      "source": [
        "##### Check GPU allocated on Google colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBxbjjjsqXJr"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMz8UU3-Nez7"
      },
      "source": [
        "#### Step 1. Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicio9FLPv5j"
      },
      "source": [
        "!pip install urllib3==1.25.10\n",
        "\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "\n",
        "# # Use language modeling version as of April 21st.\n",
        "!git checkout b1ff0b2ae7d368b7db3a8a8472a29cc195d278d8\n",
        "\n",
        "!pip install .\n",
        "!pip install -r ./examples/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiZqyDhMH043"
      },
      "source": [
        "##### [Note: If you see errors during the above installation step, restarting the runtime might help.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYVih7yYmmi7"
      },
      "source": [
        "#### Step 2. Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weh0BoPfk1zc"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/transformers/examples/')\n",
        "\n",
        "# from changed directory\n",
        "import run_language_modeling  \n",
        "import run_generation\n",
        "\n",
        "# standard ML imports\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelWithLMHead\n",
        "\n",
        "# Text processing\n",
        "import json\n",
        "from pathlib import Path\n",
        "from glob import glob\n",
        "import os\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from itertools import chain\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8PJVPNXmY92"
      },
      "source": [
        "#### Step 3. Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHQJYzKzZiIF"
      },
      "source": [
        "# I am running on Colab with data stored in Google drive.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MIoc4RXT0k"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "##### Using sliding window of 8 sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA_0gVDRTBKY"
      },
      "source": [
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/polish.pickle')\n",
        "sent_tokenize = sentence_tokenizer.tokenize\n",
        "\n",
        "\n",
        "def flatten(iterable):\n",
        "    return chain.from_iterable(iterable)\n",
        "\n",
        "\n",
        "def preprocess_book(book_txt):\n",
        "    start_idx = book_txt.index(\"START OF THIS PROJECT GUTENBERG\") + 100\n",
        "    end_idx = book_txt.index(\"END OF THIS PROJECT\") - 20\n",
        "    txt =  book_txt[start_idx: end_idx]\n",
        "    return re.sub(\"\\s+\", \" \", txt)\n",
        "\n",
        "\n",
        "def process_book(book_path):\n",
        "    try:\n",
        "        txt = preprocess_book(Path(book_path).read_text(\"utf-8\"))\n",
        "        sentences = [s for s in sent_tokenize(txt) if len(s) >= 16]\n",
        "        windowed_sentences = []\n",
        "        for snt in range(len(sentences)):\n",
        "            windowed_sentences.append(\" \".join(sentences[snt: snt + 8]))\n",
        "        return windowed_sentences\n",
        "    except:\n",
        "        print(f\"Could not parse \\n{book_path}\\n\")\n",
        "        return []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJMdA9R1bWkG"
      },
      "source": [
        "# Uncomment on first run ONLY. Once you have the training file, comment it out again.\n",
        "\n",
        "# train_data_directory = '/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/all_novels/'\n",
        "# sliding_train_data = '/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/train_sliding.txt'\n",
        "\n",
        "# books = []\n",
        "# for filename in os.listdir(train_data_directory):\n",
        "#   file_path = os.path.join(train_data_directory, filename)\n",
        "#   books.append(file_path)\n",
        "\n",
        "# buffer, BUFFER_SIZE = [], 100000\n",
        "# with open(sliding_train_data, \"w\") as file:\n",
        "#   for i, sentence in enumerate(flatten(process_book(f) for f in books)):\n",
        "#     if len(buffer) >= BUFFER_SIZE:\n",
        "#       file.write(\"\\n\".join(buffer))\n",
        "#       buffer.clear()\n",
        "#       print(i, end=\"\\r\")\n",
        "#     buffer.append(sentence)\n",
        "#   if len(buffer) > 0:\n",
        "#     file.write(\"\\n\".join(buffer))\n",
        "#     buffer.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbNQ-dOgdUWu"
      },
      "source": [
        "!head /content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMLLJSU7nmqu"
      },
      "source": [
        "# number of lines, words, characters respectively\n",
        "\n",
        "!wc /content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33GutF1QVEV"
      },
      "source": [
        "## TAKES LOOOONG TIME. DO NOT RUN ONCE you have a trained model handy.\n",
        "\n",
        "# !python run_language_modeling.py \\\n",
        "#     --output_dir='/content/drive/My Drive/finetuned_models/wodehouse' \\\n",
        "#     --model_type=gpt2 \\\n",
        "#     --model_name_or_path=gpt2-medium \\\n",
        "#     --save_total_limit=5 \\\n",
        "#     --num_train_epochs=1.0 \\\n",
        "#     --do_train \\\n",
        "#     --evaluate_during_training \\\n",
        "#     --logging_steps=500 \\\n",
        "#     --save_steps=1500 \\\n",
        "#     --train_data_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/train_sliding.txt \\\n",
        "#     --do_eval \\\n",
        "#     --eval_data_file=/content/drive/My\\ Drive/Colab\\ Notebooks/wodehouse_generator/data/validate.txt \\\n",
        "#     --per_gpu_train_batch_size=2 \\\n",
        "#     --per_gpu_eval_batch_size=2 \\\n",
        "#     --block_size=128 \\\n",
        "#     --gradient_accumulation_steps=5 \\\n",
        "#     --overwrite_output_dir # too lazy to delete previous failed run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9mAiosB2wBm"
      },
      "source": [
        "### Compute perplexity of a dataset.\n",
        "This section shows how to compute perplexity of a dataset according to either the pre-trained or your fine-tuned language model. While this is possible to do by calling `run_language_modeling.py` on the command-line as above, we'll instead call the Python functions directly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiqSgGhtcDNd"
      },
      "source": [
        "#### Look at what checkpoints are available\n",
        "Run `ls` to look at what checkpoints saved been saved. You'll want to set `CHECKPOINT_PATH` below to one of these in order to evaluate the model weights saved in that checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_qHytBIETo"
      },
      "source": [
        "!ls '/content/drive/My Drive/finetuned_models/wodehouse'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRFwV1Ry3Evk"
      },
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc2VCFBG3pFf"
      },
      "source": [
        "def load_model(args):\n",
        "  \"\"\"Creates a model and loads in weights for it.\"\"\"\n",
        "  config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  model = AutoModelWithLMHead.from_pretrained(\n",
        "      args.model_name_or_path,\n",
        "      from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "      config=config,\n",
        "      cache_dir=None\n",
        "  )\n",
        "  \n",
        "  model.to(args.device)\n",
        "  return model\n",
        "\n",
        "def set_seed(seed):\n",
        "  \"\"\"Set the random seed.\"\"\"\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  if args.n_gpu > 0:\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "def do_perplexity_eval(args, model, data_file_path):\n",
        "  \"\"\"Computes the perplexity of the text in data_file_path according to the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  args.eval_data_file=data_file_path\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "\n",
        "  result = run_language_modeling.evaluate(args, model, tokenizer, prefix=\"\")\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClE2Px-j9bb"
      },
      "source": [
        "#### How is the trained model doing?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKsGfPtzJo6b"
      },
      "source": [
        "class DictToObject(object):\n",
        "\n",
        "    def __init__(self, dictionary):\n",
        "        def _traverse(key, element):\n",
        "            if isinstance(element, dict):\n",
        "                return key, DictToObject(element)\n",
        "            else:\n",
        "                return key, element\n",
        "\n",
        "        objd = dict(_traverse(k, v) for k, v in dictionary.items())\n",
        "        self.__dict__.update(objd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERCKSncEBYgJ"
      },
      "source": [
        "# Set this to the checkpoint you want to evalute, or to \"gpt2-medium\" to\n",
        "# evaluate the pre-trained model without finetuning.\n",
        "CHECKPOINT_PATH = '/content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/finetuned_models/wodehouse/output_checkpoint_15000'\n",
        "\n",
        "# Set this to the list of text files you want to evaluate the perplexity of.\n",
        "DATA_PATHS = [\"/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/validate.txt\",\n",
        "              \"/content/drive/My Drive/Colab Notebooks/wodehouse_generator/data/test.txt\"]\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "args = collections.defaultdict(\n",
        "  model_name_or_path=CHECKPOINT_PATH,\n",
        "  output_dir=OUTPUT_PATH,\n",
        "  block_size = 128,\n",
        "  local_rank=-1,\n",
        "  eval_batch_size=2,\n",
        "  per_gpu_eval_batch_size=2,\n",
        "  n_gpu=n_gpu,\n",
        "  mlm=False,\n",
        "  device=device,\n",
        "  line_by_line=False,\n",
        "  overwrite_cache=None,\n",
        "  model_type='gpt2',\n",
        "  seed=42,\n",
        ")\n",
        "args = DictToObject(args)\n",
        "\n",
        "model = load_model(args)\n",
        "\n",
        "for data_path in DATA_PATHS:\n",
        "  eval_results = do_perplexity_eval(args, model, data_path)\n",
        "  perplexity = eval_results['perplexity']\n",
        "  print('{} is the perplexity of {} according to {}'.format(\n",
        "      perplexity, data_path, CHECKPOINT_PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5o7v2hmhMTO"
      },
      "source": [
        "### Generate samples\n",
        "The following code generates text samples that are are continuations of a provided prompt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcvySe_wrCWh"
      },
      "source": [
        "def generate_samples(args, model, prompt_text):\n",
        "  \"\"\"Generating sampling for the provided prompt using the provided model.\"\"\"\n",
        "  set_seed(args.seed)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=None)\n",
        "\n",
        "  requires_preprocessing = args.model_type in run_generation.PREPROCESSING_FUNCTIONS.keys()\n",
        "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  encoded_prompt = encoded_prompt.to(args.device)\n",
        "\n",
        "  output_sequences = model.generate(\n",
        "      input_ids=encoded_prompt,\n",
        "      max_length=args.length + len(encoded_prompt[0]),\n",
        "      temperature=args.temperature,\n",
        "      top_k=args.k,\n",
        "      top_p=args.p,\n",
        "      repetition_penalty=args.repetition_penalty,\n",
        "      do_sample=True,\n",
        "      num_return_sequences=args.num_return_sequences,\n",
        "  )\n",
        "\n",
        "  # Remove the batch dimension when returning multiple sequences\n",
        "  if len(output_sequences.shape) > 2:\n",
        "    output_sequences.squeeze_()\n",
        "\n",
        "  generated_sequences = []\n",
        "\n",
        "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "\n",
        "    # Decode text\n",
        "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # Remove all text after the stop token\n",
        "    text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
        "\n",
        "    # Remove the excess text that was used for pre-processing\n",
        "    text = text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
        "\n",
        "    # Add the prompt at the beginning of the sequence.\n",
        "    total_sequence = prompt_text + text\n",
        "\n",
        "    generated_sequences.append(total_sequence)\n",
        "\n",
        "  return generated_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3LKo9VVjHw0"
      },
      "source": [
        "def generate_wodehouse_samples(prompt):\n",
        "\n",
        "  # You should try out other prompts as well as no prompt at all.\n",
        "  PROMPT = prompt\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  n_gpu = torch.cuda.device_count()\n",
        "  print(\"Running on device: \", device)\n",
        "\n",
        "  args = collections.defaultdict(\n",
        "    model_name_or_path=CHECKPOINT_PATH,\n",
        "    output_dir=OUTPUT_PATH,\n",
        "    n_gpu=n_gpu,\n",
        "    mlm=False,\n",
        "    device=device,\n",
        "    model_type='gpt2',\n",
        "    seed=42,\n",
        "    stop_token=None, # Set this if your dataset has a special word that indicates the end of a text.\n",
        "    temperature=1.0,  # temperature sampling. Set this to temperature=1.0 to not use temperature.\n",
        "    k=50,  # k for top-k sampling. Set this to k=0 to not use top-k.\n",
        "    p=1.0,  # p for nucleus sampling. Set this to p=1.0 to not use nucleus sampling.\n",
        "    repetition_penalty=None,\n",
        "    length=900,  # Number of tokens to generate.\n",
        "    num_return_sequences=3,  # Number of independently computed samples to generate.\n",
        "  )\n",
        "  args = DictToObject(dict(args))\n",
        "\n",
        "  model = load_model(args)\n",
        "  sequences = generate_samples(args, model, PROMPT)\n",
        "\n",
        "  return sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENOalkcbqo3o"
      },
      "source": [
        "def print_sequences(sequences):\n",
        "\n",
        "  for idx, sequence in enumerate(sequences):\n",
        "  print('\\n====== GENERATION {} ======'.format(idx))\n",
        "  print(sequence)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGFeDb23LNJO"
      },
      "source": [
        "sequences = generate_wodehouse_samples(\"Seated with his wife at breakfast on the veranda which overlooked the rolling lawns and leafy woods of his charming Sussex home, Geoffrey Windlebird, the great financier, was enjoying the morning sun to the full. \")\n",
        "print_sequences(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkWgdXIZ-shY",
        "outputId": "a00d21e7-07d2-4e3a-a232-6350e79aa945",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sequences = generate_wodehouse_samples(\"It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s.\")\n",
        "print_sequences(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/23/2020 21:16:17 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/config.json\n",
            "10/23/2020 21:16:17 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/23/2020 21:16:17 - INFO - transformers.modeling_utils -   loading weights file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/pytorch_model.bin\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on device:  cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "10/23/2020 21:16:30 - INFO - transformers.configuration_utils -   loading configuration file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/config.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   Model name '/content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming '/content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   Didn't find file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/added_tokens.json. We won't load it.\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/vocab.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/merges.txt\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/special_tokens_map.json\n",
            "10/23/2020 21:16:30 - INFO - transformers.tokenization_utils -   loading file /content/drive/My Drive/finetuned_models/wodehouse/checkpoint-15000/tokenizer_config.json\n",
            "10/23/2020 21:16:30 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "====== GENERATION 0 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. He was feeling that strange exhilaration, the thrill which comes to those who have done well at school, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores.\n",
            "All he had to do was to hang about and wait, and soon the other fellow would get busy and do it at him, and it would be his triumph. As for me, I would merely sit there. He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. He was feeling that strange exhilaration, the thrill which comes to those who have done well at school, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores. He was feeling that strange exhilaration, the triumphal glow which leads down the aisle at the Savoy and reaches even to the children’s section of department stores. It was only now that gradually he began to entertain tentative hopes that I might be of any assistance.\n",
            "As for me, I would merely sit there. He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that gradually he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray.\n",
            "He had shown me hitherto nothing but a bleak future. In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that slowly he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray. The atmosphere of the place was heavy with the air of one with great affairs to discuss, and I was finding it difficult to follow what the manager was saying and to make eye- contact. With me, on the other hand, it seemed as if he were talking to himself. “And to look at you,” he proceeded as he reached for his tea, “I’m like a young Piccadilly toke. He has got no time for yapping at me.’llty old Bingo.’s got no time for yapping at you. And I’d have rather given up the struggle than have got engaged to _you_, if my lordship’s got his eye.\n",
            "In fact, at the present moment I rather fancy that my existence in the neighbourhood would be a trifle dull. It seemed to me that, in addition to boredom, the spectacle of Baxter would increase the natural tenderness with which the thing happened to him. It was only now that slowly he began to entertain tentative hopes that I might be of any assistance. And, when we passed into the inner office of the _Cosy Moments_ on Riverside Drive on the following day, I saw him walking with the manager, carrying his lunch tray. The atmosphere of the place was heavy with the air of one with great affairs to discuss, and I was finding it difficult to follow what the manager was saying and to make eye- contact. With me, on the other hand, it seemed as if he were talking to himself. “And to look at you,” he proceeded as he reached for his tea, “I’m like a young Piccadilly toke. He\n",
            "\n",
            "====== GENERATION 1 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. “Gimme dat, Liz!” He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it.\n",
            "“Gimme dat, Liz!” He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing.\n",
            "He waved a hand towards the stricken lady. “A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene.\n",
            "“A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food.\n",
            "“A couple,” he explained. He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food. He took one of these little chats without speaking. There was a deep state of melancholy about Sally which he was trying not to show. His mind seemed in a confused state at his elbow, as if he were playing a deaf game.\n",
            "He did not go into great detail, but in his opinion the matter was one that called for explanation. Sally was in an exalted mood. She might be able to fight the craving, but it seemed that she must come to some conclusion to it. “What do you mean by it?” “Well, what do you want? Why should I keep my love for you hidden?” He produced another cigar from his pocket and relit it. One of these little chats were always the least bit embarrassing. It was a familiar scene. No girl ever told me that any man in her life had ever shown the slightest sign that he intended to propose in exchange for food. She took one of these little chats without speaking. “What do you mean by it?�\n",
            "\n",
            "====== GENERATION 2 ======\n",
            "It was in Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed its expressionless stolidity. He seemed to regard me as something quite ordinary and uninteresting which he could invite to browse among his sundry wares. “What colour is that, Ukridge?” demanded the proprietor. You would mistake it for a rose, I suppose.” It might have been a rose, Ukridge supposed; but his eyes narrowed, as he contemplated the peculiar shape of one of those roses.\n",
            "Ukridge was in the street in front of Selfridge’s shop. Standing among a growing collection of elderly artists, he perceived me, as I entered, as a trifle dishevelled—a fact which, he, as we shook hands awkwardly, was due to the cold weather and the closeness of the hour, for I had only just returned from Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed its expressionless stolidity. He seemed to regard me as something quite ordinary and uninteresting which he could invite to browse among his sundry wares. “What colour is that, Ukridge?” demanded the proprietor. You would mistake it for a rose, I suppose.” There was a pause of some moments, during which Ukridge looked out of the window, stared a little blankly in perplexity into space, and finally spoke. “What colour is that, Mr. Ukridge?” “Oh, that was the colour of my hair.” Ukridge hesitated for a moment, then, with that peculiar delicacy of which he is so notoriously capable, uttered at a venture the words “You know me,” and waved me down with a sort of paternal gravity; for Ukridge, though he knew not me, was certainly a man of action. That night, at dinner, he had shown a tendency to sit up quite late, and he did so again now. It was, he told me, because he seemed to be dying to see his niece, whom he had not seen much of since her arrival, appear unexpectedly in person. He was feeling aggrieved and baffled. He had just received a telegram from my aunt describing my engagement to her, and it seemed to him that she would be glad to see me. At the moment when this young man had introduced himself to me, in the High Street before the grocer’s, he had seemed, from his pale, square face, almost cheerful; but there had been something troubling in his demeanour that night. When we shook hands, his pallor had changed; and a hard look had come into his eyes, which were set with that grim determination which marks the man of action. I have described. “Hallo, old horse!” he was saying, looking at me. I don’t know what sort of a dress this is.\n",
            "Standing among a growing collection of elderly artists, he perceived me, as I entered, as a trifle dishevelled—a fact which, he, as we shook hands awkwardly, was due to the cold weather and the closeness of the hour, for I had only just returned from Oxford Street at the hour when women come up from the suburbs to shop; and he was standing among the dogs and commissionaires outside Selfridge’s. In the background, there sounded the faintest rustle. When he perceived it, Ukridge spun round and looked inquiringly at the object of his search—the proprietor of the shop. “Who wants the new car?” inquired the proprietor, in tones of friendly interest. Ukridge’s face resumed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p7pDQ5pnHCb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}